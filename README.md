# ğŸ“Œ NLP Dataset Preparation & Model Fine-Tuning  

## ğŸ“– Overview  
This project demonstrates **Natural Language Processing (NLP)** workflows using the **IMDB Movie Reviews dataset**.  

### Key Features  
- ğŸ“‚ Dataset selection & preprocessing  
- âœ¨ Zero-shot classification with **FLAN-T5**  
- ğŸ”§ Fine-tuning & evaluation with **DistilBERT**  
- ğŸ›  Troubleshooting Hugging Face issues  

---

## ğŸ“‚ Repository Structure  
NLP-Dataset-Prep/
â”£ ğŸ“œ NLP_Assignment_Report.pdf # Report/Documentation
â”£ ğŸ“œ NLP_&_Dataset_Prep.ipynb # Main Jupyter Notebook
â”£ ğŸ“œ requirements.txt # Required dependencies
â”— ğŸ“œ README.md # Project documentation

yaml
Copy
Edit

---

## âš™ï¸ Setup Instructions  

### 1ï¸âƒ£ Clone the repository  
```bash
git clone https://github.com/mayank8868/-NLP-Dataset-Prep.git
cd NLP-Dataset-Prep
2ï¸âƒ£ Install dependencies
bash
Copy
Edit
pip install -r requirements.txt
3ï¸âƒ£ Run the notebook
Open NLP_&_Dataset_Prep.ipynb in Jupyter Notebook or VS Code and execute cells step by step.

ğŸ“Š Dataset Details
Source: IMDB Movie Reviews

Classes: Positive / Negative sentiment

Total Size: 50,000 reviews (25k train / 25k test)

Split Used:

90% Training

10% Validation

25k Test

ğŸš€ Models Implemented
ğŸ”¹ FLAN-T5 (Zero-Shot Prompting)
Uses natural language prompts for classification

No training required

Evaluated on a sample of 200 reviews

ğŸ”¹ DistilBERT (Fine-Tuned)
Lightweight BERT model fine-tuned on IMDB dataset

Evaluated with accuracy, precision, recall, and F1-score

ğŸ“ˆ Results
Model	Accuracy	F1 Score
FLAN-T5 (Zero-Shot, 200 samples)	~0.72	~0.70
DistilBERT (Fine-Tuned)	~0.91	~0.91

ğŸ›  Troubleshooting Notes
âš ï¸ CUDA Out of Memory â†’ Reduce batch size

âš ï¸ Tokenizer mismatch â†’ Ensure model & tokenizer checkpoint match

âš ï¸ Slow training â†’ Enable fp16=True for mixed precision

ğŸ™Œ Author
ğŸ‘¤ Mayank Yadav

