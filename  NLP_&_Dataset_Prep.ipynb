{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9152ffd8",
   "metadata": {},
   "source": [
    "\n",
    "# NLP & ML (Hugging Face)\n",
    "\n",
    "**Author:** _MAYANK YADAV_  \n",
    "**Date:** _20 Aug 2025_\n",
    "\n",
    "This notebook completes the:  \n",
    "1) **Dataset selection & preprocessing** (IMDB reviews)  \n",
    "2) **Prompt engineering & model interaction** (FLAN-T5)  \n",
    "3) **Fine-tuning & evaluation** (DistilBERT)  \n",
    "4) **Troubleshooting** (common issues & fixes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd583278",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment Setup\n",
    "\n",
    "Install required libraries. Re-run the cell if the install was interrupted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e99d9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)]\n",
      "Platform: Windows-11-10.0.22631-SP0\n",
      "Torch: 2.5.1+cu121\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys, platform, torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Torch:\", torch.__version__ if 'torch' in globals() else \"not installed\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91b3d2",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & Reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "549622c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer,\n",
    "                          pipeline)\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import evaluate\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "SEED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901173c5",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dataset: IMDB Movie Reviews (Hugging Face Datasets)\n",
    "\n",
    "- **Source:** `imdb` dataset from Hugging Face Datasets (`datasets.load_dataset(\"imdb\")`)  \n",
    "- **Why IMDB?** Binary sentiment labels (**positive/negative**), widely used benchmark, diverse and noisy real-world reviews.  \n",
    "- **Size:** 25k train / 25k test reviews. We will create a **validation split** from train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15fbfede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  \" see this article:\"\n",
      "Generating train split: 100%|█████████████████████████████████████████| 25000/25000 [00:00<00:00, 410608.88 examples/s]\n",
      "Generating test split: 100%|██████████████████████████████████████████| 25000/25000 [00:00<00:00, 501389.54 examples/s]\n",
      "Generating unsupervised split: 100%|██████████████████████████████████| 50000/50000 [00:00<00:00, 372799.85 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "imdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9e8ea",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Train / Validation Split\n",
    "\n",
    "We take 10% of the training set as validation (stratified by default shuffle).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8311992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 2500, 25000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "imdb = imdb.shuffle(seed=SEED)\n",
    "split = imdb[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
    "imdb_train = split[\"train\"]\n",
    "imdb_val = split[\"test\"]\n",
    "imdb_test = imdb[\"test\"]\n",
    "len(imdb_train), len(imdb_val), len(imdb_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56737f7a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Preprocessing (Cleaning, Tokenization)\n",
    "\n",
    "**Cleaning applied:**\n",
    "- Lowercasing\n",
    "- Remove HTML tags (e.g., `<br />`), URLs\n",
    "- Collapse repeated whitespace\n",
    "\n",
    "> We **do not** remove punctuation/stopwords aggressively because modern tokenizers handle them well and excessive cleaning can remove useful sentiment cues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5472e69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I just saw this at the Venice Film Festival, and can\\'t quite decide about it. We were never allowed to get close enough to any of the characters to care about them. Maybe that was the point, that we are all in a \"bubble\" of our own, but these people didn\\'t compel me to be concerned about them or shocked at their various fates. At a running time of just over an hour, the characters weren\\'t very well developed. Lots of time was devoted to shots of factory equipment (forklifts, conveyor belts, shovels); and the slightly-creepy-looking baby dolls with surprisingly lifelike eyes, that most of the characters made for a living, were somehow more interesting than the live people. An interesting experiment, but somehow it never quite came together.',\n",
       " 'i just saw this at the venice film festival, and can\\'t quite decide about it. we were never allowed to get close enough to any of the characters to care about them. maybe that was the point, that we are all in a \"bubble\" of our own, but these people didn\\'t compel me to be concerned about them or shocked at their various fates. at a running time of just over an hour, the characters weren\\'t very well developed. lots of time was devoted to shots of factory equipment (forklifts, conveyor belts, shovels); and the slightly-creepy-looking baby dolls with surprisingly lifelike eyes, that most of the characters made for a living, were somehow more interesting than the live people. an interesting experiment, but somehow it never quite came together.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)          # HTML tags\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)  # URLs\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()   # normalize spaces\n",
    "    return text\n",
    "\n",
    "# Preview cleaning\n",
    "sample = imdb_train[0][\"text\"]\n",
    "sample, clean_text(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ebfc7c",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Tokenization\n",
    "\n",
    "We use **DistilBERT (uncased)** tokenizer with max length 256 and padding/truncation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "333207a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  \" see this article:\"\n",
      "Map: 100%|██████████████████████████████████████████████████████████████| 22500/22500 [00:06<00:00, 3304.52 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 2500/2500 [00:00<00:00, 3300.37 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████| 25000/25000 [00:07<00:00, 3384.70 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'input_ids': [101,\n",
       "  1045,\n",
       "  2074,\n",
       "  2387,\n",
       "  2023,\n",
       "  2012,\n",
       "  1996,\n",
       "  7914,\n",
       "  2143,\n",
       "  2782,\n",
       "  1010,\n",
       "  1998,\n",
       "  2064,\n",
       "  1005,\n",
       "  1056,\n",
       "  3243,\n",
       "  5630,\n",
       "  2055,\n",
       "  2009,\n",
       "  1012,\n",
       "  2057,\n",
       "  2020,\n",
       "  2196,\n",
       "  3039,\n",
       "  2000,\n",
       "  2131,\n",
       "  2485,\n",
       "  2438,\n",
       "  2000,\n",
       "  2151,\n",
       "  1997,\n",
       "  1996,\n",
       "  3494,\n",
       "  2000,\n",
       "  2729,\n",
       "  2055,\n",
       "  2068,\n",
       "  1012,\n",
       "  2672,\n",
       "  2008,\n",
       "  2001,\n",
       "  1996,\n",
       "  2391,\n",
       "  1010,\n",
       "  2008,\n",
       "  2057,\n",
       "  2024,\n",
       "  2035,\n",
       "  1999,\n",
       "  1037,\n",
       "  1000,\n",
       "  11957,\n",
       "  1000,\n",
       "  1997,\n",
       "  2256,\n",
       "  2219,\n",
       "  1010,\n",
       "  2021,\n",
       "  2122,\n",
       "  2111,\n",
       "  2134,\n",
       "  1005,\n",
       "  1056,\n",
       "  4012,\n",
       "  11880,\n",
       "  2033,\n",
       "  2000,\n",
       "  2022,\n",
       "  4986,\n",
       "  2055,\n",
       "  2068,\n",
       "  2030,\n",
       "  7135,\n",
       "  2012,\n",
       "  2037,\n",
       "  2536,\n",
       "  26417,\n",
       "  1012,\n",
       "  2012,\n",
       "  1037,\n",
       "  2770,\n",
       "  2051,\n",
       "  1997,\n",
       "  2074,\n",
       "  2058,\n",
       "  2019,\n",
       "  3178,\n",
       "  1010,\n",
       "  1996,\n",
       "  3494,\n",
       "  4694,\n",
       "  1005,\n",
       "  1056,\n",
       "  2200,\n",
       "  2092,\n",
       "  2764,\n",
       "  1012,\n",
       "  7167,\n",
       "  1997,\n",
       "  2051,\n",
       "  2001,\n",
       "  7422,\n",
       "  2000,\n",
       "  7171,\n",
       "  1997,\n",
       "  4713,\n",
       "  3941,\n",
       "  1006,\n",
       "  9292,\n",
       "  18412,\n",
       "  2015,\n",
       "  1010,\n",
       "  16636,\n",
       "  2953,\n",
       "  18000,\n",
       "  1010,\n",
       "  24596,\n",
       "  2015,\n",
       "  1007,\n",
       "  1025,\n",
       "  1998,\n",
       "  1996,\n",
       "  3621,\n",
       "  1011,\n",
       "  17109,\n",
       "  1011,\n",
       "  2559,\n",
       "  3336,\n",
       "  14421,\n",
       "  2007,\n",
       "  10889,\n",
       "  2166,\n",
       "  10359,\n",
       "  2159,\n",
       "  1010,\n",
       "  2008,\n",
       "  2087,\n",
       "  1997,\n",
       "  1996,\n",
       "  3494,\n",
       "  2081,\n",
       "  2005,\n",
       "  1037,\n",
       "  2542,\n",
       "  1010,\n",
       "  2020,\n",
       "  5064,\n",
       "  2062,\n",
       "  5875,\n",
       "  2084,\n",
       "  1996,\n",
       "  2444,\n",
       "  2111,\n",
       "  1012,\n",
       "  2019,\n",
       "  5875,\n",
       "  7551,\n",
       "  1010,\n",
       "  2021,\n",
       "  5064,\n",
       "  2009,\n",
       "  2196,\n",
       "  3243,\n",
       "  2234,\n",
       "  2362,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    texts = [clean_text(t) for t in batch[\"text\"]]\n",
    "    return tokenizer(texts, truncation=True, padding=False, max_length=256)\n",
    "\n",
    "tokenized_train = imdb_train.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val   = imdb_val.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test  = imdb_test.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "tokenized_train[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043deeeb",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Fine-Tuning a Lightweight Model (DistilBERT)\n",
    "\n",
    "We fine-tune **DistilBERT** for binary sentiment classification.  \n",
    "To reduce compute, you can subsample:\n",
    "- Uncomment `select(range(8000))` to train on 8k examples (faster).  \n",
    "- Increase `num_train_epochs` for better accuracy if you have a GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6330f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20556\\1690249780.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 22:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.335800</td>\n",
       "      <td>0.283175</td>\n",
       "      <td>0.879500</td>\n",
       "      <td>0.844071</td>\n",
       "      <td>0.924335</td>\n",
       "      <td>0.882382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.178800</td>\n",
       "      <td>0.329232</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.879346</td>\n",
       "      <td>0.887513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.25729830932617187, metrics={'train_runtime': 1343.1763, 'train_samples_per_second': 11.912, 'train_steps_per_second': 0.745, 'total_flos': 1059739189248000.0, 'train_loss': 0.25729830932617187, 'epoch': 2.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Reduce dataset size for faster training (optional)\n",
    "tokenized_train = tokenized_train.select(range(8000))\n",
    "tokenized_val   = tokenized_val.select(range(2000))\n",
    "\n",
    "# Label mappings\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Training arguments (latest HF API uses evaluation_strategy)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-imdb-checkpoints\",\n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425148ec",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Evaluate on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ca55075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 10:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2973911166191101,\n",
       " 'eval_accuracy': 0.902,\n",
       " 'eval_precision': 0.9068825910931174,\n",
       " 'eval_recall': 0.896,\n",
       " 'eval_f1': 0.9014084507042254,\n",
       " 'eval_runtime': 654.084,\n",
       " 'eval_samples_per_second': 38.221,\n",
       " 'eval_steps_per_second': 1.196,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_metrics = trainer.evaluate(tokenized_test)\n",
    "test_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fab29",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Save & Reload Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54b90d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was absolutely wonderful, I loved every minute of it! -> [{'label': 'POSITIVE', 'score': 0.9937892556190491}]\n",
      "It was a total waste of time. The plot made no sense. -> [{'label': 'NEGATIVE', 'score': 0.9870553016662598}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.save_model(\"distilbert-imdb-model\")\n",
    "tokenizer.save_pretrained(\"distilbert-imdb-model\")\n",
    "\n",
    "# Quick sanity-check inference\n",
    "clf = pipeline(\"sentiment-analysis\", model=\"distilbert-imdb-model\", tokenizer=\"distilbert-imdb-model\", device=0 if torch.cuda.is_available() else -1)\n",
    "examples = [\n",
    "    \"The movie was absolutely wonderful, I loved every minute of it!\",\n",
    "    \"It was a total waste of time. The plot made no sense.\"\n",
    "]\n",
    "for ex in examples:\n",
    "    print(ex, \"->\", clf(ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e09e0c",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Prompt Engineering & Model Interaction (FLAN-T5)\n",
    "\n",
    "We use a pretrained **instruction-tuned** LLM: `google/flan-t5-base` for text-to-text prompting.  \n",
    "We design **three prompt variants** for the same sentiment task:\n",
    "\n",
    "1. **Direct Question** – concise classification request.  \n",
    "2. **Brief Reasoning** – ask for a short explanation, then final label.  \n",
    "3. **Role Prompt** – set an expert role to encourage consistency.\n",
    "\n",
    "We'll compare accuracy across prompts on a small sample of the IMDB **test** set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f6cf823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'direct',\n",
       "  'accuracy': 0.935,\n",
       "  'precision': 0.9278350515463918,\n",
       "  'recall': 0.9375,\n",
       "  'f1': 0.9326424870466321},\n",
       " {'prompt': 'brief_reason',\n",
       "  'accuracy': 0.925,\n",
       "  'precision': 0.945054945054945,\n",
       "  'recall': 0.8958333333333334,\n",
       "  'f1': 0.9197860962566845},\n",
       " {'prompt': 'role',\n",
       "  'accuracy': 0.93,\n",
       "  'precision': 0.9361702127659575,\n",
       "  'recall': 0.9166666666666666,\n",
       "  'f1': 0.9263157894736842}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "GEN_MODEL = \"google/flan-t5-base\"\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL)\n",
    "\n",
    "text2text = pipeline(\"text2text-generation\", model=gen_model, tokenizer=gen_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "PROMPTS = {\n",
    "    \"direct\": \"Classify the sentiment of this movie review as Positive or Negative. Review: {text}\",\n",
    "    \"brief_reason\": \"Explain briefly and then answer strictly with Positive or Negative. Review: {text}\",\n",
    "    \"role\": \"You are an expert movie critic. Decide if the sentiment is Positive or Negative. Review: {text}\"\n",
    "}\n",
    "\n",
    "def normalize_label(s):\n",
    "    s = s.strip().lower()\n",
    "    if \"positive\" in s:\n",
    "        return 1\n",
    "    if \"negative\" in s:\n",
    "        return 0\n",
    "    # fallback: try to guess based on polarity words\n",
    "    return 1 if any(w in s for w in [\"good\", \"great\", \"excellent\", \"love\", \"amazing\"]) else 0\n",
    "\n",
    "# Evaluate on a small sample for speed\n",
    "N = 200\n",
    "sample_test = imdb_test.select(range(N))\n",
    "\n",
    "def eval_prompt(prompt_key):\n",
    "    preds, gold = [], []\n",
    "    for ex in sample_test:\n",
    "        text = ex[\"text\"]\n",
    "        lbl = ex[\"label\"]\n",
    "        prompt = PROMPTS[prompt_key].format(text=text)\n",
    "        out = text2text(prompt, max_new_tokens=16, do_sample=False)[0][\"generated_text\"]\n",
    "        preds.append(normalize_label(out))\n",
    "        gold.append(lbl)\n",
    "    acc = accuracy_score(gold, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(gold, preds, average=\"binary\")\n",
    "    return {\"prompt\": prompt_key, \"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "results = [eval_prompt(k) for k in PROMPTS.keys()]\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0d73a",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Sample Prompt Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29b80997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review: The cinematography was stunning and the performances were heartfelt.\n",
      "      direct -> Positive\n",
      "brief_reason -> Positive\n",
      "        role -> Positive\n",
      "\n",
      "Review: Boring, predictable, and painfully long.\n",
      "      direct -> Negative\n",
      "brief_reason -> Negative\n",
      "        role -> Negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "samples = [\n",
    "    \"The cinematography was stunning and the performances were heartfelt.\",\n",
    "    \"Boring, predictable, and painfully long.\"\n",
    "]\n",
    "for s in samples:\n",
    "    print(\"\\nReview:\", s)\n",
    "    for name, tmpl in PROMPTS.items():\n",
    "        out = text2text(tmpl.format(text=s), max_new_tokens=16, do_sample=False)[0][\"generated_text\"]\n",
    "        print(f\"{name:>12} ->\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbb343",
   "metadata": {},
   "source": [
    "\n",
    "# 6. Troubleshooting (Common Issues & Fixes)\r\n",
    "\r\n",
    "## I) Overfitting during fine-tuning\r\n",
    "**Symptoms:** Training accuracy/metrics improve while validation/test stagnate or degrade.  \r\n",
    "**Fixes:** Increase dropout/weight decay; early stopping; reduce epochs; use more data; data augmentation.\r\n",
    "\r\n",
    "## II) Prompt sensitivity / drift\r\n",
    "**Symptoms:** LLM outputs vary with small wording changes; inconsistent labels.  \r\n",
    "**Fixes:** Constrain output format (e.g., answer strictly with Positive or Negative), add few-shot exemplars, temperature=0, use regex post-processing.\r\n",
    "\r\n",
    "## III) Domain bias\r\n",
    "**Symptoms:** Model favors popular movie tropes or short reviews.  \r\n",
    "**Fixes:** Balance dataset, include domain-specific examples, calibrate thresholds, evaluate on multiple slices.\r\n",
    "\r\n",
    "## IV) Class imbalance\r\n",
    "**Symptoms:** High accuracy but low recall for minority class.  \r\n",
    "**Fixes:** Weighted loss, stratified sampling, report precision/recall/F1 alongside accuracy.\r\n",
    "\r\n",
    "## V) Resource constraints\r\n",
    "**Symptoms:** Out-of-memory, slow training.  \r\n",
    "**Fixes:** Reduce max sequence length, batch size; gradient accumulation; smaller model (distilbert, tiny-llama); use GPU/AMP.\r\n",
    "lama); use GPU/AMP.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1882c419",
   "metadata": {},
   "source": [
    "## 7. Summary Notes\r\n",
    "\r\n",
    "### Project Overview  \r\n",
    "This project focuses on **sentiment analysis of IMDB reviews** using transformer-based models.  \r\n",
    "The primary objective is to classify reviews into **positive** or **negative** sentiment by applying modern NLP techniques and comparing fine-tuned models with prompting approaches.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Dataset  \r\n",
    "- **Source:** IMDB reviews (binary classification) dataset from Hugging Face.  \r\n",
    "- **Size:** 50,000 labeled reviews (balanced between positive and negative).  \r\n",
    "- **Usage:** Training, validation, and testing subsets created.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Preprocessing  \r\n",
    "- Converted all text to lowercase.  \r\n",
    "- Removed HTML tags, URLs, and extra spaces.  \r\n",
    "- Applied **BERT tokenizer** with maximum sequence length of `256`.  \r\n",
    "- Created PyTorch datasets and dataloaders for training and evaluation.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Model  \r\n",
    "- Base model: **DistilBERT (lightweight BERT variant)**.  \r\n",
    "- Fine-tuned for **2 epochs** on the preprocessed IMDB dataset.  \r\n",
    "- Optimizer: AdamW, with learning rate scheduling.  \r\n",
    "- Loss function: Cross-entropy loss.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Evaluation Metrics  \r\n",
    "On the test dataset, the following metrics were computed:  \r\n",
    "- **Accuracy**  \r\n",
    "- **Precision**  \r\n",
    "- **Recall**  \r\n",
    "- **F1-score**  \r\n",
    "\r\n",
    "These provide a complete performance evaluation beyond just accuracy.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Prompting Experiments  \r\n",
    "In addition to fine-tuning, experimented with **zero-shot prompting** using **FLAN-T5**:  \r\n",
    "- Tested with **200 random reviews**.  \r\n",
    "- Used variations of prompts: direct questions, brief reasoning, and role-based instructions.  \r\n",
    "- Compared performance with fine-tuned DistilBERT.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Compromises & Constraints  \r\n",
    "- Subsampled the dataset for faster iteration.  \r\n",
    "- Used **lightweight transformer models** (DistilBERT, FLAN-T5) to ensure feasibility on limited hardware.  \r\n",
    "- Reduced number of epochs to balance accuracy and training time.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Reproducibility  \r\n",
    "- Random seed set to `42` for consistent results.  \r\n",
    "- Code cells numbered sequentially in the Jupyter Notebook.  \r\n",
    "- Environment details (Python version, libraries, GPU/CPU availability) specified for replication.  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "This summary provides a **clear, self-contained overview** of the entire workflow—from dataset to preprocessing, model training, evaluation, prompting experiments, compromises, and reproducibility.\r\n",
    "ion, prompting experiments, compromises, and reproducibility.\r\n",
    "tails.  \r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc21c6f-38d7-4649-abd2-766942fda507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-gpu)",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
